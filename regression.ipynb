{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Gradiant Decent algorithm\n",
    "\n",
    "In linear regression there are $n$ input features and $1$ output feature that we want to predict. \n",
    "So visually what we're trying to do, is that we want to find a line \\_n-dimensional\\_ that \"fits\" data points. or more presicely, is as close as possible to all data points.\n",
    "\n",
    "The description above gives us an idea about what we're looking for but it can't be considered as a problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 What is the problem?\n",
    "Making a clear problem statement is as important as solving the problem.\n",
    "for that matter, let's note that we can model our input features with a vector $X^{(i)}$ in a vector space $V$ and output is a scaler $y^{(i)}$ in the field $F$ in which we define the vector space. here $V = \\mathbb{R}^n$ and $F = \\mathbb{R}$ and superscript $(i)$ indicates sample index.\n",
    "\n",
    "Now, let's assume that there is a function $h^*: V \\rightarrow F$ that maps these points to a scaler and is the function that exactly \"fits\" all datapoints. So $h^*(X^{(i)})$. Obviously it's not neccesserily linear or any other form.\n",
    "\n",
    "We define $h$ as the *hypothesis* \\_an estimation of $h^*$\\_ given the constraints that $h$ is a linear function.\n",
    "\n",
    "As it's known that every linear function can be represented with a vector of coefficients, the problem of finding $h$ is equivalent to finding it's vector of coefficients,\n",
    "which is represented by $\\Theta = [\\theta_{1},\\dots, \\theta_{n}]$ . So it's convinient to write $h_{\\Theta}$ instead of just $h$.\n",
    "\n",
    "now we're ready to write the problem statement.\n",
    "\n",
    "#### statement 1:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ which estimates $h^*$.\n",
    "\n",
    "From the terms \"as close as possible\" (1.) and \"estimates\" (1.2) it's not very clear what we should do. In order to define a better metric for that, which means how good some function $h_{\\Theta}$ is we define a cost function $J : V \\rightarrow F$ as:\n",
    "$$ J(\\Theta) = \\frac{1}{2m} \\sum_{i=0}^m (h_{\\Theta}(X^{(i)})-y^{(i)})^2 $$\n",
    "So we want to find some $h_{\\Theta}$ which minimizes the value of $J$.\n",
    "\n",
    "now we can update the problem statement as:\n",
    "\n",
    "#### statement 2:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ for which $J(\\Theta)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions defined above\n",
    "m = 100\n",
    "X = y = theta = []\n",
    "def h(theta, X):\n",
    "    return theta.dot(X)\n",
    "\n",
    "def J(theta):\n",
    "    res = 0\n",
    "    for i in range(m):\n",
    "        res += (h(theta, X[i])-y[i])**2\n",
    "    res /= 2*m\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "We use gradiant decent algorithm to find some $\\Theta$ which is local optimum for $J$.\n",
    "\n",
    "This is an overview on how this algorithm works:\n",
    "\n",
    "Let's assume we have $\\Theta_{1}$ as first hypothesis. we can initialize this to some random vector.\n",
    "\n",
    "We claim that $J(\\Theta_{2}) \\leq J(\\Theta_{1})$ for $\\Theta_{2} = \\Theta_{1} - \\eta \\nabla J(\\Theta_{1})$.\n",
    "\n",
    "Doing this $p-1$ times we'll end up with a seqence $\\Theta_{1}, \\dots, \\Theta_{p}$ and each one is a better estimation than the previous one.\n",
    "\n",
    "Although it might not be exactly local minimum but in many applications it's close enough.\n",
    "\n",
    "\n",
    "\n",
    "$$\\nabla J(\\Theta) = \\derivative{x} J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradiant decent\n",
    "p = 100\n",
    "nabla = 1\n",
    "def gradiant(theta):\n",
    "    gradiant_vector = []\n",
    "    return gradiant_vector\n",
    "    \n",
    "def gradiant_decent():\n",
    "    for i in range(p):\n",
    "        theta -= nabla*gradiant(theta)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
