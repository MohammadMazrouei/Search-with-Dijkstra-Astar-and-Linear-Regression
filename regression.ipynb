{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Gradiant Decent algorithm\n",
    "\n",
    "In linear regression there are $n$ input features and $1$ output feature that we want to predict. \n",
    "So visually what we're trying to do, is that we want to find a line \\_n-dimensional\\_ that \"fits\" data points. or more presicely, is as close as possible to all data points.\n",
    "\n",
    "The description above gives us an idea about what we're looking for but it can't be considered as a problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 What is the problem?\n",
    "Making a clear problem statement is as important as solving the problem.\n",
    "for that matter, let's note that we can model our input features with a vector $X^{(i)}$ in a vector space $V$ and output is a scaler $y^{(i)}$ in the field $F$ in which we define the vector space. here $V = \\mathbb{R}^n$ and $F = \\mathbb{R}$ and superscript $(i)$ indicates sample index.\n",
    "\n",
    "Now, let's assume that there is a function $h^*: V \\rightarrow F$ that maps these points to a scaler and is the function that exactly \"fits\" all datapoints. So $h^*(X^{(i)})$. Obviously it's not neccesserily linear or any other form.\n",
    "\n",
    "We define $h$ as the *hypothesis* \\_an estimation of $h^*$\\_ given the constraints that $h$ is a linear function.\n",
    "\n",
    "As it's known that every linear function can be represented with a vector of coefficients, the problem of finding $h$ is equivalent to finding it's vector of coefficients,\n",
    "which is represented by $\\Theta = [\\theta_{1},\\dots, \\theta_{n}]$ . So it's convinient to write $h_{\\Theta}$ instead of just $h$.\n",
    "\n",
    "now we're ready to write the problem statement.\n",
    "\n",
    "#### statement 1:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ which estimates $h^*$.\n",
    "\n",
    "From the terms \"as close as possible\" (1.) and \"estimates\" (1.2) it's not very clear what we should do. In order to define a better metric for that, which means how good some function $h_{\\Theta}$ is we define a cost function $J : V \\rightarrow F$ as:\n",
    "$$ J(\\Theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\Theta}(X^{(i)})-y^{(i)})^2 $$\n",
    "So we want to find some $h_{\\Theta}$ which minimizes the value of $J$.\n",
    "\n",
    "now we can update the problem statement as:\n",
    "\n",
    "#### statement 2:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ for which $J(\\Theta)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(self, x):\n",
    "    res = 0\n",
    "    # add fake feature\n",
    "    x.append(1)\n",
    "    for i in range(self.n):\n",
    "        res += x[i]*self.theta[i]\n",
    "    return res\n",
    "\n",
    "def J(self):\n",
    "    res = 0\n",
    "    for i in range(self.m):\n",
    "        res += (self.h(self.X[i])-self.y[i])**2\n",
    "    res /= 2*self.m\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "We use gradiant decent algorithm to find some $\\Theta$ which is local optimum for $J$.\n",
    "\n",
    "This is an overview on how this algorithm works:\n",
    "\n",
    "Let's assume we have $\\Theta_{1}$ as first hypothesis. we can initialize this to some random vector.\n",
    "\n",
    "We claim that $J(\\Theta_{2}) \\leq J(\\Theta_{1})$ for $\\Theta_{2} = \\Theta_{1} - \\eta \\nabla J(\\Theta_{1})$.\n",
    "\n",
    "Doing this $p-1$ times we'll end up with a seqence $\\Theta_{1}, \\dots, \\Theta_{p}$ and each one is a better estimation than the previous one.\n",
    "\n",
    "\n",
    "Next we calculate gradiant of cost funcion this way:\n",
    "$$\n",
    "\\nabla J(\\Theta) = [\\frac{\\partial}{\\partial \\theta_{1}} J, \\dots,  \\frac{\\partial}{\\partial \\theta_{n}} J]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} J = \\frac{1}{m} \\sum_{i=1}^m(h_{\\Theta}(X^{(i)}) - y^{i})X^{(i)}_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiant(self):\n",
    "    gradiant_vector = [0]*self.n\n",
    "    for j in range(self.n):\n",
    "        for i in range(self.m):\n",
    "            gradiant_vector[j] += (self.h(self.X[i])-self.y[i])*self.X[i][j]\n",
    "        gradiant_vector[j] /= self.m\n",
    "    return gradiant_vector\n",
    "    \n",
    "def gradiant_decent(self):\n",
    "    for i in range(self.p):\n",
    "        for j in range(self.n):\n",
    "            self.theta[j] -= self.nabla*self.gradiant()[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Wrap up\n",
    "This is everything put together in Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    n = m = p = eta = 0\n",
    "    X = Y = theta = []\n",
    "    \n",
    "    def __init__(self, input_dataset, output_dataset, number_of_iterations, learning_rate):\n",
    "        self.X = input_dataset\n",
    "        self.Y = output_dataset\n",
    "        self.m = len(self.X)\n",
    "        # add fake feature\n",
    "        for i in range(self.m):\n",
    "            self.X[i].append(1)\n",
    "        self.n = len(self.X[0])\n",
    "        self.p = number_of_iterations\n",
    "        self.eta = learning_rate\n",
    "        self.theta = [0]*self.n\n",
    "\n",
    "    def h(self, x):\n",
    "        res = 0\n",
    "        for j in range(self.n):\n",
    "            res += x[j]*self.theta[j]\n",
    "        return res\n",
    "\n",
    "    def J(self):\n",
    "        res = 0\n",
    "        for i in range(self.m):\n",
    "            res += (self.h(self.X[i])-self.Y[i])**2\n",
    "        res /= 2*self.m\n",
    "        return res\n",
    "\n",
    "    def gradiant(self):\n",
    "        nabla = [0]*self.n\n",
    "        for i in range(self.m):\n",
    "            val = (self.h(self.X[i])-self.Y[i])\n",
    "            for j in range(self.n):\n",
    "                nabla[j] += val*self.X[i][j]\n",
    "        for j in range(self.n):\n",
    "            nabla[j] /= self.m\n",
    "        return nabla\n",
    "        \n",
    "    def gradiant_decent(self):\n",
    "        for k in range(self.p):\n",
    "            nabla = self.gradiant()\n",
    "            for j in range(self.n):\n",
    "                self.theta[j] -= self.eta*nabla[j]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x.append(1)\n",
    "        res = 0\n",
    "        for j in range(self.n):\n",
    "            res += self.theta[j]*x[j]\n",
    "        return res\n",
    "            \n",
    "    def run(self):\n",
    "        self.gradiant_decent()\n",
    "\n",
    "    \n",
    "    def test(self, X_test, Y_test):\n",
    "        y_hat = [0]*len(X_test)\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat[i] = reg.predict(X_test[i])\n",
    "            \n",
    "        print(y_hat)\n",
    "        print(mean_absolute_error(y_hat, Y_test))\n",
    "        print(math.sqrt(mean_squared_error(y_hat, Y_test)))\n",
    "        print(r2_score(y_hat, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing with dataset\n",
    "Next, we use a dataset to test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/Downloads/Flight_Price_Dataset_Q2.csv\")\n",
    "departure_time_mapping = {\n",
    "    \"Early_Morning\": 0,\n",
    "    \"Morning\": 1,\n",
    "    \"Afternoon\": 2,\n",
    "    \"Night\": 3, \n",
    "    \"Late_Night\": 4\n",
    "}\n",
    "stops_mapping = {\n",
    "    \"zero\": 0,\n",
    "    \"one\": 1,\n",
    "    \"two_or_more\": 2\n",
    "}\n",
    "class_mapping = {\n",
    "    \"Economy\": 0,\n",
    "    \"Business\": 1\n",
    "}\n",
    "df[\"departure_time\"] = df[\"departure_time\"].map(departure_time_mapping)\n",
    "df[\"stops\"] = df[\"stops\"].map(stops_mapping)\n",
    "df[\"arrival_time\"] = df[\"arrival_time\"].map(departure_time_mapping)\n",
    "df[\"class\"] = df[\"class\"].map(class_mapping)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "Y = df[\"price\"]\n",
    "X = df.drop(\"price\", axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "number_of_iterations = 100\n",
    "x = [\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4]\n",
    "]\n",
    "y = [1, 2, 3, 4]\n",
    "xt = [\n",
    "    [5],\n",
    "    [6],\n",
    "    [7],\n",
    "    [8],\n",
    "    [9]\n",
    "]\n",
    "yt = [5, 6, 7, 8, 9]\n",
    "# reg = Regression(x, y, number_of_iterations, learning_rate)\n",
    "\n",
    "reg = Regression(X_train.values.tolist(), Y_train.values.tolist(), number_of_iterations, learning_rate)\n",
    "reg.run()\n",
    "print(\"train finished\")\n",
    "# test(xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Regression' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 58\u001b[0m, in \u001b[0;36mRegression.test\u001b[0;34m(X_test, Y_test, tmp)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(X_test, Y_test, tmp):\n\u001b[0;32m---> 58\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test)):\n\u001b[1;32m     60\u001b[0m         y_hat[i] \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mpredict(X_test[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Regression' has no len()"
     ]
    }
   ],
   "source": [
    "reg.test(X_test.values.tolist(), Y_test.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train.values.tolist(), Y_train.values.tolist())\n",
    "y_pred = regr.predict(X_test.values.tolist())\n",
    "print(mean_absolute_error(Y_test.values.tolist(), Y_pred))\n",
    "print(math.sqrt(mean_squared_error(Y_test.values.tolist(), Y_pred)))\n",
    "print(r2_score(y_test.values.tolist(), Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
