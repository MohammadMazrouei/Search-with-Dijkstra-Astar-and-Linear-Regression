{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using Gradiant Decent algorithm\n",
    "\n",
    "In linear regression there are $n$ input features and $1$ output feature that we want to predict. \n",
    "So visually what we're trying to do, is that we want to find a line \\_n-dimensional\\_ that \"fits\" data points. or more presicely, is as close as possible to all data points.\n",
    "\n",
    "The description above gives us an idea about what we're looking for but it can't be considered as a problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 What is the problem?\n",
    "Making a clear problem statement is as important as solving the problem.\n",
    "for that matter, let's note that we can model our input features with a vector $X^{(i)}$ in a vector space $V$ and output is a scaler $y^{(i)}$ in the field $F$ in which we define the vector space. here $V = \\mathbb{R}^n$ and $F = \\mathbb{R}$ and superscript $(i)$ indicates sample index.\n",
    "\n",
    "Now, let's assume that there is a function $h^*: V \\rightarrow F$ that maps these points to a scaler and is the function that exactly \"fits\" all datapoints. So $h^*(X^{(i)})$. Obviously it's not neccesserily linear or any other form.\n",
    "\n",
    "We define $h$ as the *hypothesis* \\_an estimation of $h^*$\\_ given the constraints that $h$ is a linear function.\n",
    "\n",
    "As it's known that every linear function can be represented with a vector of coefficients, the problem of finding $h$ is equivalent to finding it's vector of coefficients,\n",
    "which is represented by $\\Theta = [\\theta_{1},\\dots, \\theta_{n}]$ . So it's convinient to write $h_{\\Theta}$ instead of just $h$.\n",
    "\n",
    "now we're ready to write the problem statement.\n",
    "\n",
    "#### statement 1:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ which estimates $h^*$.\n",
    "\n",
    "From the terms \"as close as possible\" (1.) and \"estimates\" (1.2) it's not very clear what we should do. In order to define a better metric for that, which means how good some function $h_{\\Theta}$ is we define a cost function $J : V \\rightarrow F$ as:\n",
    "$$ J(\\Theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\Theta}(X^{(i)})-y^{(i)})^2 $$\n",
    "So we want to find some $h_{\\Theta}$ which minimizes the value of $J$.\n",
    "\n",
    "now we can update the problem statement as:\n",
    "\n",
    "#### statement 2:\n",
    "> Given value of $h^*$ for m points/vectors $X^{(1)}, \\dots,X^{(m)}$,  find a linear function $h_{\\Theta}$ for which $J(\\Theta)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(self, x):\n",
    "    res = 0\n",
    "    # add fake feature\n",
    "    x.append(1)\n",
    "    for i in range(self.n):\n",
    "        res += x[i]*self.theta[i]\n",
    "    return res\n",
    "\n",
    "def J(self):\n",
    "    res = 0\n",
    "    for i in range(self.m):\n",
    "        res += (self.h(self.X[i])-self.y[i])**2\n",
    "    res /= 2*self.m\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "We use gradiant decent algorithm to find some $\\Theta$ which is local optimum for $J$.\n",
    "\n",
    "This is an overview on how this algorithm works:\n",
    "\n",
    "Let's assume we have $\\Theta_{1}$ as first hypothesis. we can initialize this to some random vector.\n",
    "\n",
    "We claim that $J(\\Theta_{2}) \\leq J(\\Theta_{1})$ for $\\Theta_{2} = \\Theta_{1} - \\eta \\nabla J(\\Theta_{1})$.\n",
    "\n",
    "Doing this $p-1$ times we'll end up with a seqence $\\Theta_{1}, \\dots, \\Theta_{p}$ and each one is a better estimation than the previous one.\n",
    "\n",
    "\n",
    "Next we calculate gradiant of cost funcion this way:\n",
    "$$\n",
    "\\nabla J(\\Theta) = [\\frac{\\partial}{\\partial \\theta_{1}} J, \\dots,  \\frac{\\partial}{\\partial \\theta_{n}} J]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} J = \\frac{1}{m} \\sum_{i=1}^m(h_{\\Theta}(X^{(i)}) - y^{i})X^{(i)}_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiant(self):\n",
    "    gradiant_vector = [0]*self.n\n",
    "    for j in range(self.n):\n",
    "        for i in range(self.m):\n",
    "            gradiant_vector[j] += (self.h(self.X[i])-self.y[i])*self.X[i][j]\n",
    "        gradiant_vector[j] /= self.m\n",
    "    return gradiant_vector\n",
    "    \n",
    "def gradiant_decent(self):\n",
    "    for i in range(self.p):\n",
    "        for j in range(self.n):\n",
    "            self.theta[j] -= self.nabla*self.gradiant()[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Wrap up\n",
    "This is everything put together in Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, number_of_iterations, learning_rate):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def h(self, x):\n",
    "        res = 0\n",
    "        for j in range(self.m):\n",
    "            res += x[j]*self.theta[j]\n",
    "        return res\n",
    "\n",
    "    def J(self):\n",
    "        res = 0\n",
    "        for i in range(self.n):\n",
    "            res += (self.h(self.X[i])-self.Y[i])**2\n",
    "        res /= 2*self.n\n",
    "        return res\n",
    "\n",
    "    def gradiant(self):\n",
    "        nabla = [0]*self.m\n",
    "        for i in range(self.n):\n",
    "            val = (self.h(self.X[i])-self.Y[i])\n",
    "            for j in range(self.m):\n",
    "                nabla[j] += val*self.X[i][j]\n",
    "        for j in range(self.m):\n",
    "            nabla[j] /= self.n\n",
    "        return nabla\n",
    "        \n",
    "    def gradiant_decent(self):\n",
    "        for k in range(self.number_of_iterations):\n",
    "            nabla = self.gradiant()\n",
    "            for j in range(self.m):\n",
    "                self.theta[j] -= self.learning_rate*nabla[j]\n",
    "            \n",
    "    def run(self, input_dataset, output_dataset):\n",
    "        self.X = input_dataset\n",
    "        self.Y = output_dataset\n",
    "        # n(number of data), m(number of features)\n",
    "        self.n = len(self.X)\n",
    "        self.m = len(self.X[0])\n",
    "        \n",
    "        # add fake feature\n",
    "        for i in range(self.n):\n",
    "            self.X[i].append(1)\n",
    "        self.theta = [0]*self.m\n",
    "        self.gradiant_decent()\n",
    "\n",
    "    def predict(self, x):\n",
    "        x.append(1)\n",
    "        res = 0\n",
    "        for j in range(self.m):\n",
    "            res += self.theta[j]*x[j]\n",
    "        return res\n",
    "    \n",
    "    def test(self, X_test, Y_test):\n",
    "        y_hat = [0]*len(X_test)\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat[i] = reg.predict(X_test[i])\n",
    "            \n",
    "        # print(y_hat)\n",
    "        for i in range(20):\n",
    "            print(Y_test[i], y_hat[i])\n",
    "        print(self.theta)\n",
    "        print(mean_absolute_error(y_hat, Y_test))\n",
    "        print((mean_squared_error(y_hat, Y_test))**0.5)\n",
    "        print(r2_score(y_hat, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Testing with dataset\n",
    "Next, we use a dataset to test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/Downloads/Flight_Price_Dataset_Q2.csv\")\n",
    "departure_time_mapping = {\n",
    "    \"Early_Morning\": 0,\n",
    "    \"Morning\": 1,\n",
    "    \"Afternoon\": 2,\n",
    "    \"Night\": 3, \n",
    "    \"Late_Night\": 4\n",
    "}\n",
    "stops_mapping = {\n",
    "    \"zero\": 0,\n",
    "    \"one\": 1,\n",
    "    \"two_or_more\": 2\n",
    "}\n",
    "class_mapping = {\n",
    "    \"Economy\": 0,\n",
    "    \"Business\": 1\n",
    "}\n",
    "df[\"departure_time\"] = df[\"departure_time\"].map(departure_time_mapping)\n",
    "df[\"stops\"] = df[\"stops\"].map(stops_mapping)\n",
    "df[\"arrival_time\"] = df[\"arrival_time\"].map(departure_time_mapping)\n",
    "df[\"class\"] = df[\"class\"].map(class_mapping)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "Y = df[\"price\"]\n",
    "X = df.drop(\"price\", axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 6.233193 seconds\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "number_of_iterations = 100\n",
    "\n",
    "start_time = time.time()\n",
    "reg = Regression(number_of_iterations, learning_rate)\n",
    "reg.run(X_train.values.tolist(), Y_train.values.tolist())\n",
    "\n",
    "print(\"Training Time: %s seconds\" % round(time.time() - start_time, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39783 36445.00335794669\n",
      "41314 31076.133183397113\n",
      "6363 5172.50966542334\n",
      "9141 30896.343573640974\n",
      "64285 20673.67776417767\n",
      "4368 22057.943847090763\n",
      "7141 30742.793692882376\n",
      "49725 24170.56429141883\n",
      "15753 20339.14073930189\n",
      "1776 8436.408532802912\n",
      "53164 18214.05404354181\n",
      "6389 15665.708020997732\n",
      "60260 16984.24706072468\n",
      "5485 12541.890256074526\n",
      "13014 22803.748809452914\n",
      "4387 17963.66570983365\n",
      "64173 21122.317365024443\n",
      "2480 20069.506432561568\n",
      "3093 23647.010990444844\n",
      "60396 21593.958805239145\n",
      "[321.925794757819, 178.03566398713122, 659.1449918026676, 974.8384577225239, 1058.0896466300276, 127.95468690473596]\n",
      "17378.555635834604\n",
      "21763.850920432964\n",
      "-7.109620508788348\n"
     ]
    }
   ],
   "source": [
    "reg.test(X_test.values.tolist(), Y_test.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637.247101807908\n",
      "7150.029867706789\n",
      "0.8956233237552835\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train.values.tolist(), Y_train.values.tolist())\n",
    "Y_pred = regr.predict(X_test.values.tolist())\n",
    "print(mean_absolute_error(Y_test.values.tolist(), Y_pred))\n",
    "print((mean_squared_error(Y_test.values.tolist(), Y_pred))**0.5)\n",
    "print(r2_score(Y_test.values.tolist(), Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
